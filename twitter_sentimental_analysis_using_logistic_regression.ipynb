{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjBBPt7vMudbvOGSqOPAhq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjtsharma3538/twitter_sentiment_analysis_using_logistic_regression/blob/main/twitter_sentimental_analysis_using_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from os import getcwd\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNNdvFVB1bJi",
        "outputId": "66345735-8e0d-4ce1-e477-01621e8851bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "7QLnJOn91y9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import twitter_samples"
      ],
      "metadata": {
        "id": "utvR0ttV13sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select set of positive and negative tweets\n",
        "positive_tweet = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweet = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# each positive and negative will contain 5000 tweets each\n"
      ],
      "metadata": {
        "id": "antO3Ynl3jNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training an testing data classification  20% for testing and 80% for training\n",
        "test_positive = positive_tweet[4000:]\n",
        "train_positive = positive_tweet[:4000]\n",
        "test_negative = negative_tweet[4000:]\n",
        "train_negative = negative_tweet[:4000]\n",
        "\n",
        "train_x = train_positive + train_negative\n",
        "test_x = test_positive + test_negative"
      ],
      "metadata": {
        "id": "CL3YQBwU32Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the output for every tweet in training set and testing set\n",
        "\n",
        "train_y = np.append(np.ones((len(train_positive), 1)), np.zeros((len(train_negative), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_positive), 1)), np.zeros((len(test_negative), 1)), axis=0)"
      ],
      "metadata": {
        "id": "XDOcHWve5DC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_shape = \" + str(train_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8qOKVaoJYDa",
        "outputId": "1d9afbd3-e5d6-4848-ee5b-6cdf6739dde1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_shape = (8000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test_shape = \" + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjwsuDPUJpEQ",
        "outputId": "121b0d1c-71fd-4d46-b7e4-8eade28f4148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_shape = (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "9H55t9HCiy2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "wjRv80C_kv-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "sQh9L6oblChq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre processing of tweet\n",
        "\n",
        "def process(tweet):\n",
        "  stemmer = PorterStemmer()\n",
        "  stopwords_english = stopwords.words('english')\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)                     # remove stock market tickers like $GE\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)                  # remove old style retweet text\n",
        "  tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)       # remove hyperlinks\n",
        "  tweet = re.sub(r'#', '', tweet)                         # remove # sign\n",
        "\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweet_clean = []\n",
        "\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_english and word not in string.punctuation):\n",
        "      stem_word = stemmer.stem(word)\n",
        "      tweet_clean.append(stem_word)\n",
        "  return tweet_clean\n",
        "\n"
      ],
      "metadata": {
        "id": "TqBPXx1Yk5Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[0])\n",
        "print(process(train_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-t3VTRByDFs",
        "outputId": "df638239-a847-4d94-d762-ff14473eb04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frequency function , it consist of frequency of every word in positive and negative class\n",
        "\n",
        "def build_freq(tweets, ys):\n",
        "  freq={}\n",
        "  yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "  for y,tweet in zip(yslist,tweets):\n",
        "    for word in process(tweet):\n",
        "      pair = (word,y)\n",
        "      if pair in freq:\n",
        "        freq[pair]+=1\n",
        "      else:\n",
        "        freq[pair]=1\n",
        "\n",
        "  return freq"
      ],
      "metadata": {
        "id": "40xVpoZ_yT67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frequcny dictionary formation\n",
        "\n",
        "freqs=build_freq(train_x,train_y)"
      ],
      "metadata": {
        "id": "HyvRrByv9S4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bLiiDsS_T2p",
        "outputId": "09700465-eb80-4ec4-8365-50ce55021fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid function\n",
        "\n",
        "def sigmoid(x):\n",
        "  h = 1/(1 + np.exp(-x))\n",
        "  return h"
      ],
      "metadata": {
        "id": "e9uCM9hR_VKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sigmoid(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uohesIyh_rNv",
        "outputId": "76f13e1a-594b-4c9e-bbe9-b1ba6295689a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8807970779778823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extraction of input features in the form if [1,(sum of positive freq in dict of all words of tweet), (sum of negative freq in dict of all words of tweet)]\n",
        "\n",
        "def extract_features(tweets, freq):\n",
        "  tweet = process(tweets)\n",
        "  pos=0\n",
        "  neg=0\n",
        "  for word in tweet:\n",
        "    if (word,1) in freq:\n",
        "      pos+=freq[(word,1)]\n",
        "    if (word,0) in freq:\n",
        "      neg+=freq[(word,0)]\n",
        "  return [1,pos,neg]"
      ],
      "metadata": {
        "id": "Z5LS7pJ4cD0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent function , updating theta and minimising cost\n",
        "\n",
        "def gradient_descent(x, y, theta, alpha, iterations):\n",
        "  m=len(x)\n",
        "  for i in range(0,iterations):\n",
        "    z=np.dot(x,theta)\n",
        "    h=sigmoid(z)\n",
        "    J = (-1/m)*(np.dot(y.T,np.log(h)) + np.dot((1-y).T,np.log(1-h)))\n",
        "    theta = theta - (alpha/m)*np.dot(x.T, h-y)\n",
        "\n",
        "    J = float(J)\n",
        "\n",
        "    return J,theta"
      ],
      "metadata": {
        "id": "_N65hCW2h0lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(train_x),3))\n",
        "Y = train_y\n",
        "for i in range(len(train_x)):\n",
        "  X[i,:] = extract_features(train_x[i], freqs)\n",
        "J, theta = gradient_descent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "\n",
        "\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQMPjmx3j3kn",
        "outputId": "0de006d7-d1e9-4077-ec36-f1e0384f26e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.69314718.\n",
            "The resulting vector of weights is [0.0, 6.2e-07, -8.7e-07]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp1 = extract_features(train_x[0], freqs)\n",
        "print(tmp1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN0XeBIHlmej",
        "outputId": "ad055f23-df3d-4ab5-9ff8-9ea7439a8eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3133, 61]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "\n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet, freqs)\n",
        "\n",
        "    # make the prediction using x and theta\n",
        "    z = np.dot(x,theta)\n",
        "    y_pred = sigmoid(z)\n",
        "\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "\n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "\n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "# With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    y_hat = np.array(y_hat)\n",
        "    test_y = test_y.reshape(-1)\n",
        "    accuracy = np.sum((test_y == y_hat).astype(int))/len(test_x)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "4xSIfFozlw7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(tmp_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYBPCsQNmXtr",
        "outputId": "ee975bb5-afc6-4765-bd58-ed7b547100dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = input()\n",
        "result = predict_tweet(tweet, freqs, theta)\n",
        "if result > 0.5:\n",
        "  print(1)\n",
        "else :\n",
        "  print(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FivmZ109nDFi",
        "outputId": "8556ae71-410e-42af-ee3f-d526fee9100c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am a good boy\n",
            "1\n"
          ]
        }
      ]
    }
  ]
}